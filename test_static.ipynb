{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40966e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: recorded_data/20250515_091450_150998\n",
      "Error: Metadata file not found: recorded_data/20250515_091450_150998/custom_metadata.npz\n",
      "\n",
      "Failed to load any data or metadata from recorded_data/20250515_091450_150998.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_custom_data(session_folder_path):\n",
    "    \"\"\"\n",
    "    Loads the combined data and metadata from a custom recording session.\n",
    "\n",
    "    Args:\n",
    "        session_folder_path (str): Path to the session folder.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (data_array, metadata_dict)\n",
    "               - data_array (np.ndarray): The loaded data. Timestamps and TargetEvent columns might be prepended.\n",
    "               - metadata_dict (dict): The loaded metadata, including session_info and processed_column_names.\n",
    "               Returns (None, None) if loading fails at an early stage.\n",
    "\n",
    "    NOTE: As of May 2025, the .dat file now contains 12 columns per sample:\n",
    "        [EEG_Filt_1, EEG_Filt_2, EEG_Filt_3, EEG_Filt_4, EOG_Filt_1, EOG_Filt_2, EOG_Filt_3, EOG_Filt_4, RAW_EEG_1, RAW_EEG_2, RAW_EEG_3, RAW_EEG_4]\n",
    "        The last 4 columns are directly from raw_eeg_buffer[:, [0, 1, 3, 4]] at save time.\n",
    "        Timestamp and TargetEvent columns are still prepended if available in metadata, and are not affected by this change.\n",
    "    \"\"\"\n",
    "    data_filename = \"custom_combined_data.dat\"\n",
    "    metadata_filename = \"custom_metadata.npz\"\n",
    "\n",
    "    data_filepath = os.path.join(session_folder_path, data_filename)\n",
    "    metadata_filepath = os.path.join(session_folder_path, metadata_filename)\n",
    "\n",
    "    if not os.path.exists(metadata_filepath):\n",
    "        print(f\"Error: Metadata file not found: {metadata_filepath}\")\n",
    "        return None, None\n",
    "    \n",
    "    session_info_loaded = None\n",
    "    metadata_loaded = None\n",
    "    try:\n",
    "        metadata_loaded = np.load(metadata_filepath, allow_pickle=True)\n",
    "        session_info_loaded = metadata_loaded['session_info'].item()\n",
    "    except Exception as e_meta:\n",
    "        print(f\"Error loading metadata from {metadata_filepath}: {e_meta}\")\n",
    "        return None, None # Cannot proceed without metadata\n",
    "\n",
    "    if not os.path.exists(data_filepath):\n",
    "        print(f\"Error: Data file not found: {data_filepath}\")\n",
    "        return None, session_info_loaded \n",
    "\n",
    "    try:\n",
    "        num_channels = session_info_loaded['expected_columns'] \n",
    "        data_type = np.dtype(session_info_loaded['custom_data_type'])\n",
    "        data_shape_on_save = session_info_loaded.get('data_shape_on_save', 'samples_first') \n",
    "        original_column_names = list(session_info_loaded.get('column_names', [f'Ch{j+1}' for j in range(num_channels)]))\n",
    "\n",
    "        loaded_flat_data = np.fromfile(data_filepath, dtype=data_type)\n",
    "\n",
    "        if loaded_flat_data.size == 0:\n",
    "            print(\"Warning: Data file is empty.\")\n",
    "            # Return empty array matching expected original channels, plus session_info\n",
    "            empty_data = np.array([]).reshape(0, num_channels) \n",
    "            session_info_loaded['processed_column_names'] = original_column_names\n",
    "            return empty_data, session_info_loaded\n",
    "\n",
    "        processed_data = None\n",
    "        final_column_names = []\n",
    "\n",
    "        if data_shape_on_save == 'channels_first':\n",
    "            if loaded_flat_data.size % num_channels == 0:\n",
    "                num_samples_loaded_total = loaded_flat_data.size // num_channels\n",
    "                reshaped_data_channels_first = loaded_flat_data.reshape(num_channels, num_samples_loaded_total)\n",
    "                current_data_array = reshaped_data_channels_first.T  \n",
    "                final_column_names = list(original_column_names) # Start with original channel names\n",
    "\n",
    "                # Attempt to prepend timestamps and event data\n",
    "                block_ts = metadata_loaded.get('data_block_timestamps', None)\n",
    "                block_counts = metadata_loaded.get('data_block_sample_counts', None)\n",
    "                target_event_transitions = metadata_loaded.get('target_event_transitions', None)\n",
    "\n",
    "                if block_ts is not None and block_counts is not None and len(block_ts) > 0:\n",
    "                    if sum(block_counts) == current_data_array.shape[0]: # Validate counts match data length\n",
    "                        sample_timestamps = np.concatenate([np.full(int(cnt), float(ts)) for ts, cnt in zip(block_ts, block_counts)])\n",
    "                        current_data_array = np.column_stack((sample_timestamps, current_data_array))\n",
    "                        final_column_names.insert(0, \"Timestamp\")\n",
    "\n",
    "                        # If timestamps were added, try to add event data\n",
    "                        if target_event_transitions is not None and len(target_event_transitions) > 0:\n",
    "                            target_event_values = np.full(len(sample_timestamps), False, dtype=bool)\n",
    "                            current_event_state = False \n",
    "                            transition_idx = 0\n",
    "                            for i in range(len(sample_timestamps)):\n",
    "                                sample_ts_val = sample_timestamps[i]\n",
    "                                while transition_idx < len(target_event_transitions) and \\\n",
    "                                      target_event_transitions[transition_idx][0] <= sample_ts_val:\n",
    "                                    current_event_state = target_event_transitions[transition_idx][1]\n",
    "                                    transition_idx += 1\n",
    "                                target_event_values[i] = current_event_state\n",
    "                            \n",
    "                            # Insert event data after timestamp column\n",
    "                            current_data_array = np.column_stack((current_data_array[:,0], target_event_values, current_data_array[:,1:]))\n",
    "                            final_column_names.insert(1, \"TargetEvent\")\n",
    "                        else:\n",
    "                            print(\"Note: No target event transitions found in metadata or transitions array is empty.\")\n",
    "                    else:\n",
    "                        print(\"Warning: Sum of block_counts does not match data length. Timestamps/Events not prepended.\")\n",
    "                else:\n",
    "                    print(\"Note: data_block_timestamps or data_block_sample_counts not found or empty in metadata. Timestamps/Events not prepended.\")\n",
    "                \n",
    "                processed_data = current_data_array\n",
    "            else:\n",
    "                print(f\"Error: Cannot reshape data saved as 'channels_first'. Total elements ({loaded_flat_data.size}) not divisible by num_channels ({num_channels}).\")\n",
    "                return None, session_info_loaded\n",
    "        \n",
    "        else: # Assuming 'samples_first' or old format\n",
    "            if loaded_flat_data.size % num_channels == 0: \n",
    "                num_samples_loaded = loaded_flat_data.size // num_channels\n",
    "                processed_data = loaded_flat_data.reshape(num_samples_loaded, num_channels)\n",
    "                final_column_names = list(original_column_names)\n",
    "            else:\n",
    "                print(f\"Error: Cannot reshape data saved as 'samples_first'. Total elements ({loaded_flat_data.size}) not divisible by num_columns ({num_channels}).\")\n",
    "                return None, session_info_loaded\n",
    "\n",
    "        # NOTE: Now we have 4 extra columns at the end: RAW_EEG_1, RAW_EEG_2, RAW_EEG_3, RAW_EEG_4\n",
    "        # These are always the last 4 columns in both current_data_array and final_column_names\n",
    "        session_info_loaded['processed_column_names'] = final_column_names\n",
    "        return processed_data, session_info_loaded\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data from {data_filepath} or applying metadata: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, session_info_loaded\n",
    "\n",
    "# --- Configuration for loading ---\n",
    "# !!! IMPORTANT !!!\n",
    "# Replace 'YOUR_SESSION_FOLDER_HERE' with the actual name of the \n",
    "# subfolder in 'recorded_data/' that was created by the modified 'test_custom_save.py'.\n",
    "# For example: SESSION_FOLDER_PATH = \"recorded_data/20250507_103045_123456\"\n",
    "\n",
    "\n",
    "# <<< PLEASE UPDATE THIS PATH >>>\n",
    "SESSION_FOLDER_PATH = \"recorded_data/20250515_091450_150998\"\n",
    "\n",
    "print(f\"Attempting to load data from: {SESSION_FOLDER_PATH}\")\n",
    "loaded_data, session_metadata = load_custom_data(SESSION_FOLDER_PATH)\n",
    "\n",
    "if session_metadata is not None: \n",
    "    print(\"\\nSession Information (from metadata):\")\n",
    "    for key, value in session_metadata.items():\n",
    "        if key != 'processed_column_names': # Don't print this internal-use key here\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "    if loaded_data is not None:\n",
    "        print(\"\\nSuccessfully loaded data.\")\n",
    "        # Use processed_column_names from session_metadata for the header\n",
    "        display_column_names = session_metadata.get('processed_column_names', \n",
    "                                                [f'Col{i+1}' for i in range(loaded_data.shape[1])])\n",
    "        print(f\"Data shape (samples, columns): {loaded_data.shape}\")\n",
    "        print(f\"Columns: {display_column_names}\")\n",
    "        \n",
    "        if loaded_data.shape[0] > 0: \n",
    "            print(\"\\nFirst 5 rows of loaded data:\")\n",
    "            header = \" | \".join(display_column_names)\n",
    "            print(header)\n",
    "            print(\"-\" * len(header))\n",
    "            for row in loaded_data[:5, :]:\n",
    "                # Format each element in the row for display\n",
    "                formatted_row = []\n",
    "                for i, item in enumerate(row):\n",
    "                    col_name = display_column_names[i] if i < len(display_column_names) else \"\"\n",
    "                    if col_name == \"Timestamp\":\n",
    "                        formatted_row.append(f\"{item:.2f}\") # Timestamp with 2 decimal places\n",
    "                    elif isinstance(item, bool) or col_name == \"TargetEvent\":\n",
    "                        formatted_row.append(str(item))    # Boolean as True/False\n",
    "                    elif isinstance(item, float) or isinstance(item, np.floating):\n",
    "                        formatted_row.append(f\"{item:.3f}\" if not np.isnan(item) else \"NaN\") # Floats with 3 decimal places\n",
    "                    else:\n",
    "                        formatted_row.append(str(item))\n",
    "                print(\" | \".join(formatted_row))\n",
    "        else:\n",
    "            print(\"\\nData loaded, but no samples to display (data shape is 0 rows).\")\n",
    "\n",
    "        # ... (rest of the plotting code, if any, would need to be aware of new column indices)\n",
    "    else: \n",
    "        print(f\"\\nFailed to load data array from {SESSION_FOLDER_PATH}, but metadata was available.\")\n",
    "        print(\"Please check data file integrity and error messages above.\")\n",
    "else: \n",
    "    print(f\"\\nFailed to load any data or metadata from {SESSION_FOLDER_PATH}.\")\n",
    "    # ... (rest of the error messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96253f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_id: FRENZI40\n",
      "params: {'visualization_timescale': 60, 'data_save_frequency': 30, 'eeg_display_amplitudes': [-200, 200], 'visualization_on': True, 'max_streaming_time_s': 3600, 'event_ids': {'generic_event': -1, 'left_eye_gaze': 0, 'right_eye_gaze': 1, 'blink': 2, 'jaw_clench': 3, 'movement': 4}, 'event_key_bindings': {'space': -1, 'l': 0, 'r': 1, 'b': 2, 'j': 3, 'm': 4}, 'event_colors': {-1: 'red', 0: 'green', 1: 'blue', 2: 'black', 3: 'grey', 4: 'purple'}, 'num_eeg_channels': 6, 'eeg_sampling_rate': 125, 'num_imu_channels': 3, 'imu_sampling_rate': 50, 'num_ppg_channels': 3, 'ppg_sampling_rate': 25, 'num_hr_channels': 1, 'hr_sampling_rate': 1, 'num_spo2_channels': 1, 'spo2_sampling_rate': 1}\n",
      "eeg_data_size: 3213\n",
      "ppg_data_size: 540\n",
      "imu_data_size: 1275\n",
      "eeg_packetread_timestamps: [1.74759348e+09 1.74759348e+09 1.74759348e+09 1.74759348e+09\n",
      " 1.74759348e+09 1.74759348e+09 1.74759348e+09 1.74759348e+09\n",
      " 1.74759348e+09 1.74759348e+09 1.74759348e+09 1.74759348e+09\n",
      " 1.74759348e+09 1.74759348e+09 1.74759348e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09]\n",
      "eeg_fw_packet_timestamps: [None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None None None None None None None None\n",
      " None None None None None None None]\n",
      "ppg_packetread_timestamps: [1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09]\n",
      "imu_packetread_timestamps: [1.74759348e+09 1.74759348e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759349e+09 1.74759349e+09\n",
      " 1.74759349e+09 1.74759349e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759350e+09 1.74759350e+09\n",
      " 1.74759350e+09 1.74759350e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09 1.74759351e+09\n",
      " 1.74759351e+09 1.74759351e+09 1.74759351e+09]\n",
      "hr_packetread_timestamps: [1.74759349e+09 1.74759349e+09 1.74759350e+09 1.74759350e+09]\n",
      "spo2_packetread_timestamps: [1.74759349e+09 1.74759349e+09 1.74759350e+09 1.74759350e+09]\n",
      "event_timestamps: []\n"
     ]
    }
   ],
   "source": [
    "# metadata_loaded = np.load(\n",
    "#     \"recorded_data/1747593479.434677/recording_data.npz\", allow_pickle=True)\n",
    "# for key, value in metadata_loaded.items():\n",
    "#     print(f\"{key}: {value}\")\n",
    "\n",
    "# # Print average of loaded_data columns\n",
    "\n",
    "# # if loaded_data is not None:\n",
    "# #     print(\"\\nAverage of each column:\")\n",
    "# #     for i in range(loaded_data.shape[1]):\n",
    "# #         col_name = display_column_names[i] if i < len(display_column_names) else f\"Col{i+1}\"\n",
    "# #         avg_value = np.nanmean(loaded_data[:, i])  # Use nanmean to ignore NaN values\n",
    "# #         print(f\"{col_name}: {avg_value:.3f}\")\n",
    "\n",
    "# # print values in the first and second col of loaded_data if\n",
    "# # there is a change between the values of the second column from the previous row\n",
    "# if loaded_data is not None:\n",
    "#     print(\"\\nValues in the first and second columns (if second column is non-zero):\")\n",
    "#     for i in range(loaded_data.shape[0]):\n",
    "#         if loaded_data[i, 1] != 0 and loaded_data[i, 1] != loaded_data[i-1, 1]:\n",
    "#             print(f\"Row {i}: {loaded_data[i, 0]:.3f}, {loaded_data[i, 1]:.3f}\")\n",
    "#             print(f\"Previous value: {loaded_data[i-1, 1]:.3f} -> Current value: {loaded_data[i, 1]:.3f}\")\n",
    "\n",
    "\n",
    "# # plot all columns of loaded_data\n",
    "# # ignore the first column\n",
    "# # the second column is the target event, the rest are features\n",
    "# # x-axis should be the time and y-axis should be the EEG/EOG signals (3rd column and onwards)\n",
    "# # 2nd column are 0 and 1 indicating the target event\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# if loaded_data is not None:\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     for i in range(6, loaded_data.shape[1]):\n",
    "#         col_name = display_column_names[i] if i < len(display_column_names) else f\"Col{i+1}\"\n",
    "#         plt.plot(loaded_data[:, i], label=col_name, alpha=0.15)\n",
    "\n",
    "#     # plot the target event with a different color for if the value is 0 or 1\n",
    "#     target_event = loaded_data[:, 1]\n",
    "#     # plt.plot(target_event, label=\"Target Event\", color='red', alpha=0.5)\n",
    "#     plt.fill_between(range(len(target_event)), target_event, color='red', where=(target_event == 1), label=\"Target Event (filled)\")\n",
    "#     plt.fill_between(range(len(target_event)), target_event, color='blue', where=(target_event == 0), label=\"Non-Target Event (filled)\")\n",
    "#     # plt.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "#     plt.xlabel(\"Sample Index\")\n",
    "#     plt.ylabel(\"Value\")\n",
    "#     plt.title(\"All Columns of Loaded Data\")\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6374f54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['start_time', 'focus_score', 'delta', 'theta', 'alpha', 'beta', 'gamma', 'end_time', 'array__focus_score', 'array__delta', 'array__theta', 'array__alpha', 'array__beta', 'array__gamma'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from frenztoolkit.reader import load_experiment\n",
    "\n",
    "FILE_PATH = \"app/recorded_data/20250520_220319/1747793005.050953\"\n",
    "# Load the recorded session from your experiment folder\n",
    "# find the folder contains your recorded data by timestamp\n",
    "session_data = load_experiment(FILE_PATH)\n",
    "\n",
    "# Access EEG and POAS score history\n",
    "# get keys of 'DATA'\n",
    "print(session_data[\"SCORE\"].keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
